# 📝 深度学习学习日志 · 第3天

**日期**：2025年9月10日
**主题**：分类模型核心流程（Linear → 分数 → softmax → 交叉熵）与若干概念串联 + 反向传播 & Transformer 理解

---

## 📌 今天学了什么 / 串联起来的知识点

1. **线性层（Linear / 全连接）**

   * 计算是 $y = Wx + b$，本质上就是把输入做一次加权和再加偏置，输出是一组任意的数 —— 可以把它看作模型对每个类别的“打分”。
   * 线性层本身不受概率限制，给模型更多自由去学习。

2. **为什么这些分数可以变成概率**

   * 用 **softmax**：把每个分数做指数然后除以总和，就能得到非负且和为 1 的数列，可以当做概率。
   * 指数让较大的分数放得更开，分数相差多少，概率比就是指数关系。
   * **temperature** 会把分数除以一个数 $T$，控制结果是更平滑还是更尖锐。

3. **交叉熵（Cross Entropy）**

   * 用来衡量真实类别和预测概率之间的差距。
   * 当真实标签用 one-hot（只有真实类别为 1）时，交叉熵就是看模型给真实类别的概率有多大：概率越大，损失越小。

4. **为什么要做 softmax + 交叉熵**

   * softmax：把模型输出的分数变成“像概率”的东西；
   * 交叉熵：把这个概率和真实标签比较，得到一个数值作为训练的目标（损失）。
   * 在 PyTorch 里 `nn.CrossEntropyLoss` 把这两步合并了，数值更稳定，所以通常直接把线性层的原始分数传进去就行。

5. **PyTorch 实践要点**

   * `nn.CrossEntropyLoss(reduction='none')`：返回每个样本的损失，不做平均，方便单独处理每个样本。
   * 优化器例子：`torch.optim.SGD(net.parameters(), lr=0.1)` —— `lr` 是学习率，控制每步更新的大小。
   * 训练一个模型的基本流程：前向 → 计算 loss → `backward()` → `optimizer.step()` → `optimizer.zero_grad()`。

6. **反向传播 & Transformer**

   * 今天把**反向传播**的原理弄清楚了——把误差沿着网络往回传，逐层算出每个参数对误差的影响，然后用这些梯度更新参数。
   * 对 **Transformer** 的工作机制也有了整体理解（自注意力、位置编码、多头机制、残差连接与层归一化等），并把这些和自己以前玩 GPT 的经验连起来了。

7. **对深度学习的感悟**

   * “深度学习就是矩阵乘法加上可训练的系数”——感觉这个描述很贴切：大量是线性/矩阵运算，再配合非线性，最后通过训练得到参数。
   * 把零散的知识串起来后更有把握了，计划把学到的内容实际跑一遍。

---

## ❓ 今天弄明白的一些具体问题（Q\&A）

### Q1: 为什么用独热（one-hot）表示标签？

* 把标签看成一种“确定的概率分布”（真实类别为 1，其它为 0），这样方便和模型输出的概率比较，用交叉熵算损失。

### Q2: softmax 是做什么的？

* 把任意的分数转成可解释为概率的一组数（非负且总和为 1），分数越大对应的概率越高。

### Q3: 交叉熵为什么常用作分类损失？

* 它直接反映模型给真实类别的支持力度（概率越高，损失越小），而且数学上好求导，训练时表现稳定。

### Q4: 为何 Linear 输出不是概率？

* 线性层只是做加权求和+偏置，没被概率约束，这样更灵活，之后再用 softmax 把它变成概率。

### Q5: temperature 在 softmax 中有什么作用？

* 把分数除以 $T$：$\text{softmax}(o_i / T)$。

  * $T>1$ 让分布更平缓（不那么自信），
  * $T<1$ 让分布更尖锐（更偏向分数最高的类别）。

---

## ✅ 今天的收获与下一步计划

**收获**

* 把很多零散的概念（线性层的输出、softmax、one-hot、交叉熵、优化器、反向传播、Transformer、temperature）连成了一条清晰的链条，觉得自己已经入门了。
* 回想起大一用 GPT 时看到的 temperature，现在能把它放进 softmax 的框架里理解了。

**下一步计划**

1. 在本地跑通一个简单的分类模型（PyTorch），完整实现 Linear → CrossEntropyLoss → SGD 的训练循环。
2. 实验观察不同分数和不同 temperature 下，softmax 输出和交叉熵是怎么变化的。
3. 尝试把一个小模型部署在我的mac电脑上（先 CPU，要是老师分了服务器可以试个大的），把理论和实践对应起来。
