## 📝 深度学习学习日志 · 第2天

**日期**：2025年9月4日
**主题**：单层线性回归 & 梯度下降入门

---

## 📌 今日进展

1. **复习了昨天的内容**

   * 了解了深度学习的发展历史
   * 完成了深度学习实验环境的搭建（Miniconda + PyTorch + JupyterLab）

2. **今天的学习内容**

   * 掌握了单层线性回归在深度学习中的原理
   * 初步了解了四步实现线性回归的流程
   * 用 PyTorch 准备了数据集
   * 学习了梯度下降算法的基本思想
   * 弄清楚了损失函数、梯度、反向传播等核心概念

---

## ❓ 今日问答记录

### Q1: **类积是什么？损失函数干什么用？**

* “类积”应为“内积”或“矩阵乘法”。
* **损失函数**用来衡量模型预测与真实值之间的差距，指导优化。

---

### Q2: **一个线性回归就是一个训练实例吗？多个就是训练集？**

* 一个线性回归模型不是一个训练实例。
* **训练实例** = 一条输入输出样本
* **训练集** = 多条训练实例的集合
* 线性回归是用这些训练实例来学习参数。

---

### Q3: **为何要用矩阵？**

* 批量处理高效
* GPU 并行计算友好
* 代码更简洁，支持矢量化运算

---

### Q4: **为何损失函数用平方误差？**

* 数学上可导且凸，方便优化
* 对异常值敏感，能更快调整参数
* 直观解释：类似“平均偏差”的平方形式

---

### Q5: **解析解和梯度下降的区别？**

* **解析解**：一次性解方程得到最优参数（适合小规模问题）
* **梯度下降**：通过迭代更新逼近最优解（适合大规模、复杂模型）

---

### Q6: **梯度下降算法流程？**

1. 初始化参数
2. 正向传播，计算预测
3. 计算损失函数
4. 反向传播，更新参数

---

### Q7: **用 PyTorch 如何准备数据集？**

* 使用 `TensorDataset` 打包 `(X, y)`
* 用 `DataLoader` 批量迭代数据，支持 shuffle 和 batch

---

### Q8: **反向传播求导（求梯度）是什么意思？**

* 用链式法则将误差从输出层传回输入层
* 逐层计算损失对参数的偏导数
* 得到每个参数的梯度，用于更新

---

### Q9: **内循环和外循环是什么？**

* **外循环**：epoch 循环（遍历整个训练集）
* **内循环**：batch 循环（每次处理一个小批量，更新参数）

---

### Q🔟: **为什么是张量数据集？**

* 张量支持批量运算
* GPU 加速友好
* 能自动记录计算图，支持反向传播

---

## ✅ 总结

今天把 **线性回归的数学原理、梯度下降的流程** 搞明白了，也学会了 **PyTorch 的数据准备方法**。虽然内容不少，但有了前两天的环境准备，学习效率提升明显。下一次课准备把 **完整的线性回归模型代码复现** 并部署运行。
