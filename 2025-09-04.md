
# 📝 深度学习学习日志 · 第2天

**日期**：2025年9月4日
**主题**：单层线性回归 & 梯度下降入门

---

## 📌 今天学了什么

1. **先复习昨天的内容**

   * 大概了解了深度学习的历史
   * 环境已经搭建好（Miniconda + PyTorch + JupyterLab）

2. **今天新学的内容**

   * 什么是单层线性回归
   * 梯度下降大概是怎么回事
   * 用 PyTorch 做了一个小数据集
   * 知道了损失函数、梯度、反向传播这些关键词是干嘛的
   * 学会了“写一个小模型的四个基本步骤”

---

## ❓ 今天搞明白的一些问题

### Q1: “类积”是什么？损失函数又是干啥的？

* 应该是指 **内积**（向量/矩阵相乘那种）
* **损失函数**就是一个“误差打分器”，看预测和真实差多少

---

### Q2: 一个线性回归是不是就等于一个训练样本？

* 不是的，一个样本是“一条数据”，一堆样本加起来才是训练集
* 线性回归是用这些样本来学参数的

---

### Q3: 为什么要用矩阵？

* 一次能处理一堆数据，比一条一条算快多了
* 还能更好用 GPU 算

---

### Q4: 为什么损失函数常用平方误差？

* 公式简单，好求导
* 平方让误差大的地方更显眼，能更快纠正

---

### Q5: 解析解 vs 梯度下降

* **解析解**：直接算出最优答案（小问题能用，大问题就没戏）
* **梯度下降**：慢慢调参数，边走边靠近最优解

---

### Q6: 梯度下降的步骤

1. 先随便给参数一个初始值
2. 算一遍预测结果
3. 算一算误差
4. 根据误差调一调参数（不断重复）

---

### Q7: PyTorch 里怎么准备数据？

* `TensorDataset` 把 (X, y) 包起来
* `DataLoader` 一小批一小批地喂给模型

---

### Q8: 反向传播求导是啥？

* 就是“误差往回传”，一步步算每个参数对误差的影响
* 算出来的值就是 **梯度**，用来更新参数

---

### Q9: 内循环和外循环？

* **外循环**：大循环，跑完一遍数据（epoch）
* **内循环**：小循环，每次拿一小批数据（batch）

---

### Q10: 为什么用张量？

* 张量就是“高维数组”
* 能一次性算很多数据，还能让 GPU 加速
* PyTorch 还能帮忙自动算梯度

---

## ✅ 总结

今天算是把线性回归和梯度下降的基本逻辑搞懂了。虽然很多词听着还挺抽象，但串起来就能理解大概的思路。也学会了在 PyTorch 里准备数据。下一步打算真的跑一跑线性回归的完整代码，把理论和代码对应起来。
